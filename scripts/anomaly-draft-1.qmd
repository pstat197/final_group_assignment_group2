---
title: "Anomaly-detection-vignette"
format: html
editor: visual
---

# Time-Series Anomaly Detection

In this vignette, you'll learn how to conduct anomaly detection for a single time-series data.

## What is Anomaly Detection?

A time series is the sequential set of values tracked over a time duration. An anomaly is something that happens that was unexpected or was caused by an abnormal event. (more contents to go....)

## Set up 

We are going to work with "anomalize" and "timetk" packages in R.

```{r}
library(tidyverse)
library(ggplot2)
library(tibbletime)
library(anomalize)
library(timetk)

# import dataset 
processed_data <- read.csv("data/processed_data.csv")

# view dataset
head(processed_data, 5)

# data processing 
str(processed_data)

# change chr to Date format for 'DATE' column
# and select only the unemployment rate in LA
df <- processed_data %>%
  mutate(DATE, DATE = as.Date.character(DATE)) %>%
  select(DATE, unemploy_rate_la) %>% 
  as_tibble(df) # convert df to a tibble

str(df)
```

## Uni-variate Time Series Anomaly Detection

The entire process of Anomaly detection for a time-series take place across 3 steps:

1.  Decompose the time-series into the underlying variables: *trend, seasonality, remainder*
2.  Create upper and lower thresholds based on certain algorithms
3.  Identify the data points which are outside the thresholds as anomalies

```{r}
# using 'anomalize' package
#The R ‘anomalize’ package enables a workflow for detecting anomalies in data. The # main functions are time_decompose(), anomalize(), and time_recompose().

df_anomalized <- df %>%
  time_decompose(unemploy_rate_la, merge = TRUE) %>%
  anomalize(remainder) %>%
  time_recompose()

df_anomalized %>% glimpse() 
```

In the output tibble, there is a character column labeling if a time value is an anomalies or not. we can guess that anomalies are determined by "remainder" and the interval formed by "remainder_l1" and "remainder_l2". Then, we can visualize those anomalies.

```{r}
df_anomalized %>% plot_anomalies(ncol = 4, alpha_dots = 0.75)
```

What is the time period when anomalies are detected? We can observe how those anomalies lie in seasonal, trend, and remainder component.

```{r}
p1 <- df_anomalized %>%
  plot_anomaly_decomposition() +
  ggtitle("Freq/Trend = 'auto'")

p1
```

Then, we can adjust the default trend and seasonality to see what the difference. Let's check what is the default frequency trend for our seasonal decomposition method. This implies that if the scale is 1 day (meaning the difference between each data point is 1 day), then the frequency will be 7 days (or 1 week) and the trend will be around 90 days (or 3 months).

```{r}
get_time_scale_template()
```

We can adjust local parameters to see what will happen. You will find the Covid-19 period is so odd upon the whole time period. You can try to exclude years after 2019 to see the difference.

```{r}
p2 <- df %>%
  time_decompose(unemploy_rate_la,
                 frequency = "auto",
                 trend     = "6 months") %>%
  anomalize(remainder) %>%
  plot_anomaly_decomposition() +
  ggtitle("Trend = 6 months (Local)")

p2
```

Can we detect other economic recession? The answer is Yes. The [alpha]{.underline} and [max_anoms]{.underline} are the two parameters that control the *anomalize()* function. If we decrease alpha, it increases the bands making it more difficult to be an outlier. The max_anoms parameter is used to control the maximum percentage of data that can be an anomaly. Please alter two parameters to see what will output.

```{r}
# Adjusting Alpha and Max Anoms

p4 <- df %>%
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder, alpha = 0.025, max_anoms = 0.2) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("alpha = 0.05")
#> frequency = 7 days
#> trend = 91 days
p5 <- df %>%
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder, alpha = 0.6, max_anoms = 0.2) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("alpha = 0.05")
#> frequency = 7 days
#> trend = 91 days
p4
p5


```

```{r}
p6 <- df %>%
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder, alpha = 0.3, max_anoms = 0.2) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("20% Anomalies")
#> frequency = 7 days
#> trend = 91 days

p7 <- df %>%
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder, alpha = 0.3, max_anoms = 0.05) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("5% Anomalies")
#> frequency = 7 days
#> trend = 91 days
p6
p7
```

Finally, we can extract the anomalous data points.

```{r}
df %>% 
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder) %>%
  time_recompose() %>%
  filter(anomaly == 'Yes')
```

## Methods and Techniques used in "anomalize"

Anomaly detection is performed on remainders from a time series analysis that have had removed both:

-   Seasonal Components: cyclic pattern usually occurring on a daily cycle for minute or hour data. Here, the cyclic pattern can be interpreted as yearly cycles for monthly data

-   Trend Components: Longer term growth that happens over many observations

Therefore, the main goal of step 1 is to generate remainders from a time series. The seasonal decomposition outperforms ARIMA and other machine learning models

We can observe two techniques for seasonal decomposition in the "anomalize" package.

### STL: Seasonal Decomposition of Time Series by Loess

### Twitter: 

```{r}
# STL Decomposition Method
p1 <- df %>%
    time_decompose(unemploy_rate_la, 
                   method    = "stl") %>%
    anomalize(remainder) %>%
    plot_anomaly_decomposition() +
    ggtitle("STL Decomposition")
#> frequency = 7 days
#> trend = 91 days
#> Registered S3 method overwritten by 'quantmod':
#>   method            from
#>   as.zoo.data.frame zoo

# Twitter Decomposition Method
p2 <- df %>%
    time_decompose(unemploy_rate_la, 
                   method    = "twitter") %>%
    anomalize(remainder) %>%
    plot_anomaly_decomposition() +
    ggtitle("Twitter Decomposition")
#> frequency = 7 days
#> median_span = 85 days

# Show plots
p1
p2
```

### Comparison of IQR and GESD Methods

```{r}
# Generate anomalies
set.seed(100)
x <- rnorm(100)
idx_outliers    <- sample(100, size = 5)
x[idx_outliers] <- x[idx_outliers] + 10

# Visualize simulated anomalies
qplot(1:length(x), x, 
      main = "Simulated Anomalies",
      xlab = "Index") 
```

```{r}
# Analyze outliers: Outlier Report is available with verbose = TRUE
iqr_outliers <- iqr(x, alpha = 0.05, max_anoms = 0.2, verbose = TRUE)$outlier_report

gesd_outliers <- gesd(x, alpha = 0.05, max_anoms = 0.2, verbose = TRUE)$outlier_report

# ploting function for anomaly plots
ggsetup <- function(data) {
    data %>%
        ggplot(aes(rank, value, color = outlier)) +
        geom_point() +
        geom_line(aes(y = limit_upper), color = "red", linetype = 2) +
        geom_line(aes(y = limit_lower), color = "red", linetype = 2) +
        geom_text(aes(label = index), vjust = -1.25) +
        theme_bw() +
        scale_color_manual(values = c("No" = "#2c3e50", "Yes" = "#e31a1c")) +
        expand_limits(y = 13) +
        theme(legend.position = "bottom")
}
    

# Visualize
p3 <- iqr_outliers %>% 
    ggsetup() +
    ggtitle("IQR: Top outliers sorted by rank") 

p4 <- gesd_outliers %>% 
    ggsetup() +
    ggtitle("GESD: Top outliers sorted by rank") 
    
# Show plots
p3
p4
```
