---
title: "Vignette Writeup"
author: "Name"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

```{r}
# load any other packages and read data here
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(knitr)
library(tibbletime)
library(anomalize)
library(timetk)
library(zoo) #dates
library(Rbeast) #used for change-point detection
library(changepoint) #used for change-point detection
library(changepoint.np) #used for change-point nonparemetric
```

# Executive summary

Time series anomaly detection and change-point on the univariate (potentially multivariate case) for time series economic data from LA concerning unemployment.

# Data description

```{r}
unemployment <- read_csv(here::here('data/processed_data.csv'))
unemployment
```

Above we can see that the data collected is essentially a wrap-up on the 1st of a given month from the years 1900 to 2021 recorded from the city of Los Angeles. These variables encapsulate various city markers that range from unemployment to government benefits. Notably, belows are some interesting variables of interest:

-   The date (Year-Month-Day) of each observation/recording

-   The unemployment rate

-   Average price of electricity

-   Average price of gasoline

Since we are only concerned with the time-series aspect of unemployment, we are only focusing on the date and the unemployment rate of each month, therefore we will not show much interest in the other variables, yet.

# Anomaly Detection

In this vignette, you'll learn how to conduct anomaly detection for a single time-series data.

### What is Anomaly Detection?

A time series is the sequential set of values tracked over a time duration. Anomaly detection is the process of identifying unusual patterns that do not conform to expected behavior, called anomalies. Traditionally speaking, we define an anomaly or an outlier as "An observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism."Using this definition, we can further classify anomalies into two categories:

1.  **\*\*Point Outlier\*\***: A point outlier is a datum that behaves unusually in a specific time instance when compared either to the other values in the time series (global outlier), or to its neighboring points (local outlier).
2.  **\*\*Subsequences Outlier\*\***: This means consecutive points in time whose joint behavior is unusual, although each observation individually is not necessarily a point outlier. Subsequence outliers can also be global or local, and can affect one (univariate subsequence outlier) or more (multivariate subsequence outlier) time-dependent variables.

### Set up

We are going to work with "anomalize" and "timetk" packages in R.

```{r}
# import dataset 
processed_data <- read.csv(here::here("data/processed_data.csv"))

# view dataset
head(processed_data, 5)

# data processing 
str(processed_data)

# change chr to Date format for 'DATE' column
# and select only the unemployment rate in LA
df <- processed_data %>%
  mutate(DATE, DATE = as.Date.character(DATE)) %>%
  select(DATE, unemploy_rate_la) %>% 
  as_tibble(df) # convert df to a tibble

str(df)
```

### Uni-variate Time Series Anomaly Detection

The entire process of Anomaly detection for a time-series take place across 3 steps:

1.  Decompose the time-series into the underlying variables: *trend, seasonality, remainder*
2.  Create upper and lower thresholds based on certain algorithms
3.  Identify the data points which are outside the thresholds as anomalies

```{r}
# using 'anomalize' package
#The R ‘anomalize’ package enables a workflow for detecting anomalies in data. The # main functions are time_decompose(), anomalize(), and time_recompose().

df_anomalized <- df %>%
  time_decompose(unemploy_rate_la, merge = TRUE) %>%
  anomalize(remainder) %>%
  time_recompose()

df_anomalized %>% glimpse() 
```

In the output tibble, there is a character column labeling if a time value is an anomalies or not. we can see that anomalies are determined by "remainder" and the interval formed by "remainder_l1" and "remainder_l2". Then, we can visualize those anomalies.

```{r}
df_anomalized %>% plot_anomalies(ncol = 4, alpha_dots = 0.75)
```

What is the time period when anomalies are detected? We can observe how those anomalies lie in seasonal, trend, and remainder component.

```{r}
p1 <- df_anomalized %>%
  plot_anomaly_decomposition() +
  ggtitle("Freq/Trend = 'auto'")

p1
```

Then, we can adjust the default trend and seasonality to see what is the difference. Let's check what is the default frequency trend for our seasonal decomposition method. This implies that if the scale is 1 day (meaning the difference between each data point is 1 day), then the frequency will be 7 days (or 1 week) and the trend will be around 90 days (or 3 months).

```{r}
get_time_scale_template()
```

We can adjust local parameters to see what will happen. You will find the Covid-19 period is so odd upon the whole time period. You can try to exclude years after 2019 to see the difference.

```{r}
p2 <- df %>%
  time_decompose(unemploy_rate_la,
                 frequency = "auto",
                 trend     = "6 months") %>%
  anomalize(remainder) %>%
  plot_anomaly_decomposition() +
  ggtitle("Trend = 6 months (Local)")

p2
```

Can we detect other economic recession? The answer is Yes. The [alpha]{.underline} and [max_anoms]{.underline} are the two parameters that control the *anomalize()* function. If we decrease alpha, it increases the bands making it more difficult to be an outlier. The max_anoms parameter is used to control the maximum percentage of data that can be an anomaly. Please alter two parameters to see what will output.

```{r}
# Adjusting Alpha and Max Anoms

p4 <- df %>%
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder, alpha = 0.025, max_anoms = 0.2) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("alpha = 0.05")
#> frequency = 7 days
#> trend = 91 days
p5 <- df %>%
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder, alpha = 0.6, max_anoms = 0.2) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("alpha = 0.05")
#> frequency = 7 days
#> trend = 91 days
p4
p5


```

```{r}
p6 <- df %>%
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder, alpha = 0.3, max_anoms = 0.2) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("20% Anomalies")
#> frequency = 7 days
#> trend = 91 days

p7 <- df %>%
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder, alpha = 0.3, max_anoms = 0.05) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("5% Anomalies")
#> frequency = 7 days
#> trend = 91 days
p6
p7
```

Finally, we can extract the anomalous data points.

```{r}
df %>% 
  time_decompose(unemploy_rate_la) %>%
  anomalize(remainder) %>%
  time_recompose() %>%
  filter(anomaly == 'Yes')
```

### Methods and Techniques used in "anomalize"

Anomaly detection is performed on remainders from a time series analysis that have had removed both:

-   Seasonal Components: cyclic pattern usually occurring on a daily cycle for minute or hour data. Here, the cyclic pattern can be interpreted as yearly cycles for monthly data

-   Trend Components: Longer term growth that happens over many observations

Therefore, the main goal of step 1 is to generate remainders from a time series. The seasonal decomposition outperforms ARIMA and other machine learning models

We can observe two techniques for seasonal decomposition in the "anomalize" package.

#### STL: Seasonal Decomposition of Time Series by Loess

The STL method uses the `stl()` function from the `stats` package. STL works very well in circumstances where a long term trend is present. The Loess algorithm typically does a very good job at detecting the trend. However, it circumstances when the seasonal component is more dominant than the trend, Twitter tends to perform better.

#### Twitter: Seasonal Decomposition of Time Series by Median

The Twitter method works identically to STL for removing the seasonal component. The main difference is in removing the trend, which is performed by removing the median of the data rather than fitting a smoother. The median works well when a long-term trend is less dominant that the short-term seasonal component. This is because the smoother tends to overfit the anomalies.

#### Comparison of STL and Twitter Decomposition Methods

```{r}
# STL Decomposition Method
p1 <- df %>%
    time_decompose(unemploy_rate_la, 
                   method    = "stl") %>%
    anomalize(remainder) %>%
    plot_anomaly_decomposition() +
    ggtitle("STL Decomposition")
#> frequency = 7 days
#> trend = 91 days
#> Registered S3 method overwritten by 'quantmod':
#>   method            from
#>   as.zoo.data.frame zoo

# Twitter Decomposition Method
p2 <- df %>%
    time_decompose(unemploy_rate_la, 
                   method    = "twitter") %>%
    anomalize(remainder) %>%
    plot_anomaly_decomposition() +
    ggtitle("Twitter Decomposition")
#> frequency = 7 days
#> median_span = 85 days

# Show plots
p1
p2
```

We can see that the season components for both STL and Twitter decomposition are exactly the same. The difference is the trend component.

### Comparison of IQR and GESD Methods

Once a time series analysis is completed and the remainder has the desired characteristics, the remainders can be analyzed. The challenge is that anomalies are high leverage points that distort the distribution. The `anomalize` package implements two methods that are resistant to the high leverage points:

-   **IQR**: Inner Quartile Range It takes a distribution and uses the 25% and 75% inner quartile range to establish the distribution of the remainder. Limits are set by default to a factor of 3X above and below the inner quartile range, and any remainders beyond the limits are considered anomalies.

    The IQR method does not depend on any loops and is therefore faster and more easily scaled than the GESD method. However, it may not be as accurate in detecting anomalies since the high leverage anomalies can skew the centerline (median) of the IQR.

-   **GESD**: Generalized Extreme Studentized Deviate Test

    It involves an iterative evaluation of the Generalized Extreme Studentized Deviate test, which progressively evaluates anomalies, removing the worst offenders and recalculating the test statistic and critical value. The main benefit is that GESD is less resistant to high leverage points since the distribution of the data is progressively analyzed as anomalies are removed.

We can generate anomalous data to illustrate how each method work compares to each other.

```{r}
# Generate anomalies
set.seed(100)
x <- rnorm(100)
idx_outliers    <- sample(100, size = 5)
x[idx_outliers] <- x[idx_outliers] + 10

# Visualize simulated anomalies
qplot(1:length(x), x, 
      main = "Simulated Anomalies",
      xlab = "Index") 
```

For **IQR**, The `alpha` parameter adjusts the 3X factor. By default, `alpha = 0.05` for consistency with the GESD method. An `alpha = 0.025`, results in a 6X factor, expanding the limits and making it more difficult for data to be an anomaly.

For **GESD**, The `alpha` parameter adjusts the width of the critical values. By default, `alpha = 0.05`.

```{r}
# Analyze outliers: Outlier Report is available with verbose = TRUE
iqr_outliers <- iqr(x, alpha = 0.05, max_anoms = 0.2, verbose = TRUE)$outlier_report

gesd_outliers <- gesd(x, alpha = 0.05, max_anoms = 0.2, verbose = TRUE)$outlier_report

# ploting function for anomaly plots
ggsetup <- function(data) {
    data %>%
        ggplot(aes(rank, value, color = outlier)) +
        geom_point() +
        geom_line(aes(y = limit_upper), color = "red", linetype = 2) +
        geom_line(aes(y = limit_lower), color = "red", linetype = 2) +
        geom_text(aes(label = index), vjust = -1.25) +
        theme_bw() +
        scale_color_manual(values = c("No" = "#2c3e50", "Yes" = "#e31a1c")) +
        expand_limits(y = 13) +
        theme(legend.position = "bottom")
}
    

# Visualize
p3 <- iqr_outliers %>% 
    ggsetup() +
    ggtitle("IQR: Top outliers sorted by rank") 

p4 <- gesd_outliers %>% 
    ggsetup() +
    ggtitle("GESD: Top outliers sorted by rank") 
    
# Show plots
p3
p4
```

We can see that the IQR limits don't vary whereas the GESD limits get more stringent as anomalies are removed from the data. As a result, the GESD method tends to be more accurate in detecting anomalies at the expense of incurring more processing time for the looped anomaly removal. This expense is most noticeable with larger data sets (many observations or many time series).

# Change-point Detection

### What is Change-point Detection

Time series data is a widely important data type that we have been learning more as of late. This data is described as sequences of measurements over time describing the behavior of systems. Often, we'll see time series data in medicine, aerospace, finance, etc. The behaviors of the systems can change due to an aspect of time, therefore time series must be account for time in a way linear systems do not. Change point detection is a methodology to analyze data which detects abrupt changes in data when a property of the time series changes. A common example in stocks might be when do we see an abrupt change in the average prices of a certain stock over the period of a time, i.e., for one month cryptocurrency might sit at 70 dollars but then suddenly the next month it drops down to 4 and so we'd say the change-point was the period that seperates the two months.

Changes are spotted through three primary detectors:

-   mean

-   variance

-   mean-variance

Change-point will detect changes from one sequence to another sequence usually based on one of these 3 detectors to see if there's a "change."

### Examples on Time Series LA Data

Let's use data and examples to give some actual intuition about how change point works regarding time series data. I would like to note, similar to above, we will be performing this only on the univariate case, as moving to the multivariate cases increasing the difficulty by a bit. As once you consider a multivariate case, no longer can you only consider changes in time, but you must also account for covariate correlation between variables along with changes in time.

Plotting the data below you can already see some very obvious points where there are "changes" in unemployment.

```{r}
processed_data$DATE <- as.Date(processed_data$DATE, "%Y/%m/%d") # change date from characters to date element
data <- tibble(processed_data$DATE,processed_data$unemploy_rate_la) %>%
  rename("unemploy_rate_la" = "processed_data$unemploy_rate_la", "date" = "processed_data$DATE")
value.ts = ts(data$unemploy_rate_la,start = c(1990,1),end = c(2022,1), frequency = 12) 
plot(value.ts)
```

Some very obvious changes offcur at \~2008 and \~2020, so we should expect a change point to occur here.

### Packages

Some notable packages used for change-point is: -Rbeast -changepoint

These two packages are what we'll be using for the data.

Primarily, changepoint will be the more useful package since it allows more flexibility in how the parameters are set. But Rbeast is a nice package to use for seeing quick preliminaries on the data.

### Using RBeast

```{r setup, include=FALSE}
y <- data$unemploy_rate_la
out = beast(y, season='none')
```

```{r}
print(out)
plot(out)
```

To read this graph, the dotted lines mean that there are indications of a change-point. In this graph, it detects around 10, and we can tell that there are change points where our initially intuition had expected it to be. What's neat about the package is that it tells us the probability that it expects a changepoint there, so at the x-axis of time of 363 and 138, about 2020 and 2008 respectively, it detects around a 99% chance that there is a valid change point there account for mean and variance together.

### Using changepoint

How do we know if a change-point found is significant or not? We calculate cost of the whole data with no change, if the difference is large enough then we say there is no change, otherwise if we see a appropriate difference in two intervals in mean, variance, or both, then we say there's a change. In changepoint package, the default change-point metric to test if there is a change point or not is MBIC - a Modified Bayesian Information Criterion. There are various other's but MBIC is a linear penalty to find if there is a singular, at most one changepoint, in terms of time series data this isn't usually the case but let's try it out.

```{r}
m1.amoc = cpt.mean(value.ts, penalty = "MBIC")
plot(m1.amoc)
```

Notice that we definitely need more than one point to express change points. But before we proceed, when using changepoint mean, we assume that the data values follow a normal distribution with all steps from points to points having the same standard deviation, usually 1. Therefore, let's scale appropriately and see if our data may break assumptions.

```{r}
LA.scale = cpt.mean(as.vector(scale(data$unemploy_rate_la)))
plot(LA.scale)
```

The change-point doesn't change so therefore it's fairly safe to assume that our data is fairly normalized and since we still get the same changepoint after scaling the variance, that shows that this is actually a changepoint.

```{r}
m2.man = cpt.var(value.ts,method = "PELT")
plot(m2.man)
```

There are two primary methods that we cover:

-   PELT (Pruned Exact Linear Time)

-   BinSeg (Binary Segmentation)

Typically, while asymptotically PELT is quicker in performance speed, Binary Segmentation has shown to be a bit faster in terms of application. Although, PELT typically leads to substantially more accurate segmentations than Binary Segmentation

Above we see the use of PELT only with respect to variance, and we can already see some inaccuracies with an inability to track all the notable changes. This might be a sign that we should also check for mean.

```{r}
## using mean
m3.man = cpt.mean(value.ts,method = "PELT")
plot(m3.man)
```

Already, we can already see improvement using mean and that it's starting to match our results from the Rbeast preliminary runs. Using mean and PELT gives 13 change point positions, but to not draw any conclusions, lets try using mean and variance and perhaps a different penalty method to see if we can get something "better."

```{r}
## finding changepoint with respect to variance and mean
mv1.pelt <- cpt.meanvar(value.ts, method = "PELT")
plot(mv1.pelt)
# notice that PELT produces way too many points so overall it's not that useful to our analysis, therefore
# we should change our method, thus I opted for BinSeg. And we already can see that it produces much more useful 
# data than PELT does since it shows meaning points of interest.
```

From an initial view, we can already see that meanvar and PELT produces far too many change-points and that it doesn't really produce anything of value. Therefore, let's opt to use Binary Segmentation and mean, instead, which we might expect to get better points.

```{r, warning=FALSE}
mv2.pelt <- cpt.mean(value.ts, method = "BinSeg")
plot(mv2.pelt)
```

Although we see far less change-points, we can already see the most valuable point's that would be telling of significant unemployment rates changes. Such as the recession in 2008 and COVID in 2020.

Although the testing is ambiguous and it's difficult to see what really is "good" change point detection, this is how we went about our methods and intuition.
